# -*- coding: utf-8 -*-
"""automate_Ahmad-Jazil-Faiz.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lAUw9jBht6auB3-CEmXSjAAazA6MqGaZ
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
import os

def perform_preprocessing(data_path, output_folder):
    # 1. Load Data
    try:
        df = pd.read_csv(data_path)
        print(f"Data loaded successfully from {data_path}")
    except FileNotFoundError:
        print("File not found!")
        return

    # Hapus ID jika ada
    if 'id' in df.columns:
        df = df.drop(columns=['id'])

    # Hapus Duplikat (Tambahan penting)
    df = df.drop_duplicates()

    # 2. Handling Missing Values (BMI)
    df['bmi'] = pd.to_numeric(df['bmi'], errors='coerce')
    imputer = SimpleImputer(strategy='median')
    df['bmi'] = imputer.fit_transform(df[['bmi']])

    # 3. Encoding Kategorikal
    df_processed = pd.get_dummies(df, drop_first=True)

    # 4. Splitting Data
    if 'stroke' in df_processed.columns:
        X = df_processed.drop('stroke', axis=1)
        y = df_processed['stroke']
    else:
        print("Target column 'stroke' not found.")
        return

    # Split Data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # 5. Scaling
    numerical_cols = ['age', 'avg_glucose_level', 'bmi']
    available_num_cols = [col for col in numerical_cols if col in X_train.columns]

    scaler = StandardScaler()
    X_train[available_num_cols] = scaler.fit_transform(X_train[available_num_cols])
    X_test[available_num_cols] = scaler.transform(X_test[available_num_cols])

    # --- BAGIAN PENTING: MENYIMPAN HASIL KE CSV ---
    os.makedirs(output_folder, exist_ok=True)

    # Gabungkan kembali X dan y untuk disimpan (opsional, tergantung kebutuhan training nanti)
    # Atau simpan X_train, y_train terpisah. Di sini kita simpan X_train yang sudah bersih.

    # Simpan Training Set
    train_data = X_train.copy()
    train_data['stroke'] = y_train
    train_data.to_csv(f"{output_folder}/train_processed.csv", index=False)

    # Simpan Test Set
    test_data = X_test.copy()
    test_data['stroke'] = y_test
    test_data.to_csv(f"{output_folder}/test_processed.csv", index=False)

    print(f"Preprocessing completed. Files saved to {output_folder}")

if __name__ == "__main__":
    # Sesuaikan path ini dengan struktur folder GitHub Action nanti
    # Input dari folder raw
    INPUT_PATH = '/content/stroke.csv'
    # Output ke folder preprocessing result
    OUTPUT_FOLDER = 'namadataset_preprocessing'

    # Cek jika script dijalankan dari dalam folder 'preprocessing'
    if os.path.exists(INPUT_PATH):
        perform_preprocessing(INPUT_PATH, OUTPUT_FOLDER)
    else:
        # Fallback jika dijalankan dari root folder
        perform_preprocessing('namadataset_raw/stroke.csv', 'preprocessing/namadataset_preprocessing')